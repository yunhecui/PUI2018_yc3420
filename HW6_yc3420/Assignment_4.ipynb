{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This assignment is following the instructions that provided by Dr. Bianco. \n",
    "\n",
    "It is finished by Yunhe Cui as an individual task (though I use \"we\" instead of \"me\").\n",
    "\n",
    "\n",
    "# !!! Please run the spatial_join_in_py3.ipynb in a python3 environment before rerun this notebook!!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test if the distribution of \n",
    "\n",
    "#### 1) trip duration of bikers that ride during the day vs night\n",
    "\n",
    "#### 2) age of bikers for trips originating in Manhattan and in Brooklyn (extra credit)\n",
    "\n",
    "are different. Use 3 tests: KS, Pearson's, Spearman's. \n",
    "\n",
    "Use the scipy.stats functions scipy.stats.ks_2samp, scipy.stats.pearsonr, scipy.stats.spearmanr. \n",
    "\n",
    "For the KS do the test with the entire dataset and with a subset 200 times smaller\n",
    "\n",
    "Choose a single significant threshold for the whole exercise. \n",
    "\n",
    "For each test phrase the Null Hypothesis in words.\n",
    "\n",
    "Describe the return of the scipy function you use in each case.\n",
    "\n",
    "State the result in terms of rejection of the Null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#imports downloader\n",
    "from getCitiBikeCSV import getCitiBikeCSV\n",
    "import scipy.stats\n",
    "import os\n",
    "%pylab inline\n",
    "\n",
    "# the getCitiBikeCSV is retrieved from Dr. Bianco's folder in repo PUI2018/HW06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the citibike data for 2015-02 (cold month) and 2018-08 (warm month)\n",
    "datastring_1 = '201502'\n",
    "getCitiBikeCSV(datastring_1)\n",
    "datastring_2 = '201508'\n",
    "getCitiBikeCSV(datastring_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $PUIDATA\n",
    "# files are now in the PUIDATA folder as listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_02 = pd.read_csv(\"%s/201502-citibike-tripdata.csv\"%os.getenv(\"PUIDATA\"))\n",
    "df_08 = pd.read_csv(\"%s/201508-citibike-tripdata.csv\"%os.getenv(\"PUIDATA\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_02, df_08])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print (\"df_02 shape is {}\".format(df_02.shape))\n",
    "# print (\"df_08 shape is {}\".format(df_08.shape))\n",
    "# print (\"the combined df shape is {}\".format(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: trip duration of bikers that ride during the day vs night\n",
    "\n",
    "#### Since we used the ridership data from Feb and Aug, the sunrise-sunset times are different at a large scale. In this case, we define the day time as 7am to 7pm and night time from 7pm to 7am of the following day.\n",
    "H0: There is NO statistical difference in term of trip duration depending on time (day/night)   \n",
    "Ha: There is statistical difference in term of trip duration depending on time (day/night)   \n",
    "significance level α = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['starttime'])\n",
    "df['hour'] = df['date'].dt.hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop redundant columns\n",
    "df_c = df.drop([u'starttime', u'stoptime', u'start station name',u'end station name',u'bikeid',u'usertype',u'birth year', u'gender', u'date'], axis = 1)\n",
    "df_c = df_c.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the day/night attribute\n",
    "df_c['dur_day'] = df_c['tripduration'][(df_c['hour']>4) & (df_c['hour']<=17)]\n",
    "df_c['dur_night'] = df_c['tripduration'][(df_c['hour']<=4) | (df_c['hour']>17)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c = df_c.iloc[:,1:]\n",
    "#df_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the description\n",
    "df_c.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the NaN value\n",
    "df_c['dur_day'].dropna(inplace= True)\n",
    "df_c['dur_night'].dropna(inplace= True)\n",
    "df_c.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the data in histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we use 60 sec as a split bin. The maximum data is too large which could be considered as outliers. We could use 1 hour (3600 second) as the upper boundary of our dataset to show the trend of our data   \n",
    "The histograms are as follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.arange(0, 3600, 60)\n",
    "\n",
    "Day = df_c.dur_day.groupby(pd.cut(df_c.dur_day, bins)).agg([count_nonzero]).plot(kind='bar', figsize = (20,4),legend=False)\n",
    "Day.set_title(\"Day Trip Durations\")\n",
    "\n",
    "Night = df_c.dur_night.groupby(pd.cut(df_c.dur_night, bins)).agg([count_nonzero]).plot(kind='bar',figsize = (20,4),legend=False)\n",
    "Night.set_title(\"Night Trip Durations\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure1. This is a histogram of Day/Night citibike ride duration distribution. The raw data is from citibike 2015 Feb and Aug. We could see that the distribution of those day/night trip duration have similar positive skewed trend. However, the number of day ridership is more than 3 times of night ones.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print df.ageS, df.ageS.cumsum()\n",
    "\n",
    "drd=df_c.dur_day.groupby(pd.cut(df_c.dur_day, bins)).agg([count_nonzero]).cumsum()\n",
    "\n",
    "drn=df_c.dur_night.groupby(pd.cut(df_c.dur_night, bins)).agg([count_nonzero]).cumsum()\n",
    "\n",
    "print (np.abs(drd / drd.max()- drn / drn.max()))\n",
    "\n",
    "pl.plot(bins[:-1] + 5, drd / drd.max(), label = \"Day\")\n",
    "pl.plot(bins[:-1] + 5, drn / drn.max(), label = \"Night\")\n",
    "pl.plot(bins[:-1] + 5, np.sqrt(drn / drn.max() - drd / drd.max())**2, 'k-',label = \"difference\")\n",
    "pl.xlabel(\"Trip duration\")\n",
    "pl.ylabel(\"Normalized Cumulative Number\")\n",
    "pl.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure2. This chart shows the normalized day/night citibike trip duration. It is clear that the difference between the distribution of them is very small. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1-1 test to compare 2 samples\n",
    "According to https://docs.scipy.org/doc/scipy0.15.1/reference/generated/scipy.stats.ks_2samp.html, If the K-S statistic is small or the p-value is high, then we cannot reject the hypothesis that the distributions of the two samples are the same. Only the 2-sided test could be implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = scipy.stats.ks_2samp(df_c.dur_day, df_c.dur_night)\n",
    "print (ks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We got p-value is extremely small (we could see it as 0) which is definitely smaller than 0.05 (our preset significance level). In this case, we COULD REJECT the Null hypothesis which is \"There is NO statistical difference in term of trip duration depending on time (day/night)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1-2 KS test w/ reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r = df_c.sample(df_c['dur_day'].shape[0]/200, axis = 0)\n",
    "df_r['dur_day'].dropna(inplace= True)\n",
    "df_r['dur_night'].dropna(inplace= True)\n",
    "ks_r = scipy.stats.ks_2samp(df_r.dur_day, df_r.dur_night)\n",
    "print (ks_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this case, we used a random sample which is selected from the dataset (sample size  200 times smaller). The p value is 0.2701 which is larger than 0.05. Thus, we could not reject the null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scipy.stats KS test already tells me the significance and the p-value.  \n",
    "The next few cells are here just to show you how you would obtain the same result by hand, but they are not required.  \n",
    "Remember: the Null hypothesis is rejected if  \n",
    "$D_KS(n1,n2) &gt; c(\\alpha) \\sqrt{\\frac{(n1 + n2)}{n1n2}}$\n",
    "(see class notes) where $c(\\alpha$) is the inverse of the KS distribution, and you do not have to know how to get that cause there are tables that list critical values!!  \n",
    "http://www.real-statistics.com/tests-normality-and-symmetry/statistical-tests-normality-symmetry/kolmogorov-smirnov-test/kolmogorov-distribution/  \n",
    "But also this result depends in your choice of binning through, and thustheresultyou get by hand may not be exactly the same as the one the KS returns. Either way: this is how you would calculate the KS statistics by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pearson's test for correlation\n",
    "#### -> the following words are retrieved from scipy.org   \n",
    "Like other correlation coefficients, this one varies between -1 and +1 with 0 implying no correlation. Correlations of -1 or +1 imply an exact linear relationship. Positive correlations imply that as x increases, so does y. Negative correlations imply that as x increases, y decreases.  \n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html#scipy.stats.pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H0: There is NO linear relationship between Day&Night trip duration\n",
    "significance level α = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.stats.pearsonr(x , y) x,y should be the same length\n",
    "len_min = min(df_c['dur_day'].shape, df_c['dur_night'].shape)\n",
    "len_min = len_min[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Day_reduced = np.sort(df_c['dur_day'].sample(n = len_min, axis = 0))\n",
    "Night_reduced = np.sort(df_c['dur_night'].sample(n = len_min, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson = scipy.stats.pearsonr(Day_reduced,Night_reduced)\n",
    "pearson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The correlation coefficient of Pearson's test is 0.95966, which indicate a strong positive linear relationship. The p value is 0.0 and we could reject our null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Spearman's test for correlation\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html#scipy.stats.spearmanr  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.stats.spearmanr(a, b=None, axis=0, nan_policy='propagate')\n",
    "spearman = scipy.stats.spearmanr(Day_reduced,Night_reduced)\n",
    "spearman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The correlation coefficient of Spearman's test almost 1, which indicate a strong positive linear relationship. The p value is 0.0 and we could reject our null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=======================================================================\n",
    "## Problem 2: age of bikers for trips originating in Manhattan and in Brooklyn (extra credit)\n",
    "\n",
    "# !!! Please run the spatial_join_in_py3.ipynb in a python3 environment before rerun this notebook!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  H0: There is no statistical difference in the age distribution of bikers in Manhattan and Brooklyn\n",
    "#### significance level p = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['age'] = 2015 - df['birth year']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $PUIDATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough = pd.read_csv(\"%s/borough.csv\"%os.getenv(\"PUIDATA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borough = borough.rename(columns = {'StationNam': 'start station name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = pd.merge(df, borough, on='start station name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b = df_b.drop([u'starttime', u'stoptime', u'start station id',\n",
    "       u'start station name', u'start station latitude',\n",
    "       u'start station longitude', u'end station id', u'end station name',\n",
    "       u'end station latitude', u'end station longitude', u'bikeid',\n",
    "       u'usertype', u'birth year', u'gender', u'date', u'hour',\n",
    "       u'Latitude', u'Longitude', u'boro_code'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_b.sort(['boro_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_Manh = df_b.loc[(df_b['boro_name'] == 'Manhattan')]\n",
    "Df_Manh = Df_Manh.reset_index().iloc[:,1:]\n",
    "Df_Manh.dropna(inplace = True)\n",
    "Df_Manh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_Bkly = df_b.loc[(df_b['boro_name'] == 'Brooklyn')]\n",
    "Df_Bkly = Df_Bkly.reset_index().iloc[:,1:]\n",
    "Df_Bkly.dropna(inplace = True)\n",
    "Df_Bkly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The above steps are linking borough name and citibike data based on the START station name. Then we cleaned the data for conducting the correlation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data in histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_2 = np.arange(10, 99, 5)\n",
    "MN = Df_Manh.age.groupby(pd.cut(Df_Manh.age, bins_2)).agg([count_nonzero]).plot(kind='bar',legend=False)\n",
    "MN.set_title(\"Manhattan riders\")\n",
    "BK = Df_Bkly.age.groupby(pd.cut(Df_Bkly.age, bins_2)).agg([count_nonzero]).plot(kind='bar',legend=False)\n",
    "BK.set_title(\"Brooklyn riders\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure3. This is a histogram of Manhattan/Brooklyn citibike rider age distribution. The raw data is from citibike 2015 Feb and Aug. We could see that the distribution of those Manhattan/Brooklyn rider age duration have similar positive skewed trend while the Brooklyn one is more concentrate near the axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agM = Df_Manh.age.groupby(pd.cut(Df_Manh.age, bins_2)).agg([count_nonzero]).cumsum()\n",
    "\n",
    "agB = Df_Bkly.age.groupby(pd.cut(Df_Bkly.age, bins_2)).agg([count_nonzero]).cumsum()\n",
    "\n",
    "print (np.abs(agM / agM.max() - agB / agB.max()))\n",
    "\n",
    "pl.plot(bins_2[:-1] + 5, agM / agM.max(), label = \"Manhattan\")\n",
    "pl.plot(bins_2[:-1] + 5, agB / agB.max(), label = \"Brooklyn\")\n",
    "pl.plot(bins_2[:-1] + 5, np.sqrt(agB / agB.max() - agM / agM.max())**2, 'k-',label = \"difference\")\n",
    "pl.xlabel(\"Age\")\n",
    "pl.ylabel(\"Normalized Cumulative Number\")\n",
    "pl.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure2. This chart shows the normalized Manhattan/Brooklyn citibike rider age cumulative distribution. It is clear that the difference between the distribution of them is generally very small. However, in the age range 30-50, there is a relative large difference between those two categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skeleton ipynb: \n",
    "\n",
    "\"They look similar! But the difference gets to 10%. If I wanted to code the KS test by hand I woud have everything I need: the normalized cumulative distributions can be subtracted from each other and the max distance can calculated.\n",
    "Notice that there may be NaN values you are gonna have to deal with! You can do that for example with a Boolean statementsuch as df.ageF[~np.isnan(df.ageF)] or you can use numpy functions that deal with Nan values: nansum, nanmean, nanstd...\n",
    "lets run the scipy KS test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1-1 KS test to compare 2 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_2 = scipy.stats.ks_2samp(Df_Manh.age, Df_Bkly.age)\n",
    "print (ks_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We got p-value = 0 which is smaller than 0.05. In this case, we COULD REJECT the Null hypothesis which is \"There is NO statistical difference in the age distribution of bikers in Manhattan and Brooklyn\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1-2 KS test w/ reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_Manh.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Df_Bkly.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Df_Manh_r = Df_Manh.sample(1017560/200, axis = 0)\n",
    "# Df_Bkly_r = Df_Bkly.sample(67166/200, axis = 0)\n",
    "#Df_Manh.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_r_2 = scipy.stats.ks_2samp(Df_Manh.sample(1017560/200, axis = 0), Df_Bkly_r)\n",
    "print (ks_r_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this case, we used a random sample which is selected from the dataset . The p value is 0.2701 which is larger than 0.05. Thus, we could not reject the null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pearson's test for correlation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PUI2016_Python2",
   "language": "python",
   "name": "pui2016_python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
